{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: msgpack-python in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.1 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python2.7/site-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Pegah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# the imports used in A2\n",
    "import re\n",
    "# import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "# import bs4\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# Imports that might help with various functionality\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "# Additional imports from A3\n",
    "from __future__ import print_function\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# import Levenshtein  # package python-Levenshtein\n",
    "\n",
    "# Additional imports from A5\n",
    "import nltk\n",
    "# nltk.download()\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not run these 2 blocks -- Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY package\n",
    "import spacy\n",
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"nlp.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(nlp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'nlp.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e47a303c2746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpkl_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'nlp.pickle'"
     ]
    }
   ],
   "source": [
    "pkl_nlp = open('nlp.pickle', 'rb')\n",
    "nlp = pickle.load(pkl_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each woman's entry is a dictionary with the following keys...\n",
      "[u'url', u'views', u'name', u'summary']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    women_summaries = json.load(f)\n",
    "\n",
    "print(\"Each woman's entry is a dictionary with the following keys...\")\n",
    "print(women_summaries[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVED FROM SPACY VERSION\n",
    "# creates a list of women to keep as a global variable\n",
    "women_names = list()\n",
    "for i in range(len(women_summaries)):\n",
    "    name = women_summaries[i]['name']\n",
    "    end_index = name.find('(')\n",
    "    if end_index != -1 and name[: (end_index-1)] not in women_names :\n",
    "        women_names.append(name[: (end_index-1)])\n",
    "    elif end_index == -1 and name not in women_names :\n",
    "        women_names.append(name)\n",
    "        \n",
    "# print(women_names)\n",
    "# women_names is a list of the names of women in order of the JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_names = []\n",
    "deduped_women = list(women_summaries)\n",
    "for woman in deduped_women:\n",
    "    if woman['name'] not in check_names:\n",
    "        check_names.append(woman['name'])\n",
    "    else:\n",
    "        deduped_women.remove(woman)\n",
    "import pickle\n",
    "pickle.dump(deduped_women, open( \"deduped_women.pickle\", \"wb\" ), protocol=2)\n",
    "pkl_file_wmn = open('deduped_women-search.pickle', 'rb')\n",
    "deduped_women_wack = pickle.load(pkl_file_wmn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Dictionary\n",
    "For building token dictionary for W2W matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary that maps woman to the list of tokens associated with her\n",
    "# {'woman1' : ['token1', 'token2', ... 'tokenn']}\n",
    "def build_spacy_token_dictionary(input_summaries, input_women_names):\n",
    "    # Builds a dictionary that maps each woman to their tokenized words (yes we've already done this but we want the spacy version)\n",
    "    women_token_dictionary = dict()\n",
    "    \n",
    "    for i in range(len(input_women_names)):\n",
    "        name = input_women_names[i]\n",
    "        summary = input_summaries[i]['summary']\n",
    "    \n",
    "        index_was = summary.find('was ')\n",
    "        index_is = summary.find('is ')\n",
    "        index_currently = summary.find('currently ')\n",
    "        if (index_was == -1 and index_is == -1):\n",
    "            summary2 = summary[index_currently:]\n",
    "        elif (index_was == -1 and index_currently == -1):\n",
    "            summary2 = summary[index_is:]\n",
    "        elif (index_is == -1 and index_currently == -1):\n",
    "            summary2 = summary[index_was:]\n",
    "        else :\n",
    "            index_min = min(index_is, index_was)\n",
    "            summary2 = summary[index_min:]\n",
    "            \n",
    "        # lowercase long string of summary\n",
    "        summary_lower = summary2.lower()\n",
    "        # doc object type\n",
    "        summary_nlp = nlp(summary_lower)\n",
    "        # type list\n",
    "        summary_list = [token.text for token in summary_nlp]\n",
    "        # type list, but without stop words\n",
    "        summary_token_filtered = [w for w in summary_list if w not in stopwords.words('english')]\n",
    "        \n",
    "        # convert to a string separated by spaces\n",
    "        summary_str = \" \".join(summary_token_filtered) \n",
    "        summary_nlp = nlp(summary_str)\n",
    "        women_token_dictionary[name] = summary_nlp\n",
    "    \n",
    "#     print(women_token_dictionary)\n",
    "    return women_token_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT BECAUSE TOKENIZER DOES THIS\n",
    "women_token_dictionary = build_spacy_token_dictionary(women_summaries, women_names)\n",
    "# # print(women_token_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer\n",
    "#### Includes pickling the vectorizer and the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df = 0.95)\n",
    "matx = vectorizer.fit_transform(map(lambda x: x[\"summary\"], deduped_women))\n",
    "\n",
    "import pickle\n",
    "with open(\"myvectorizer.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    pickle.dump(matx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sim_matrix(input_token_dictionary, input_len_names, input_women_names):\n",
    "    # Builds matrix using spacy that stores each woman's cosine similarity to each other\n",
    "    \n",
    "    sim_matrix = np.zeros(shape = (input_len_names, input_len_names))\n",
    "    \n",
    "    for woman1 in input_token_dictionary:\n",
    "        for woman2 in input_token_dictionary:\n",
    "            woman1_index = input_women_names.index(woman1)\n",
    "            woman2_index = input_women_names.index(woman2)\n",
    "#             print(input_token_dictionary[woman1])\n",
    "#             print(input_token_dictionary[woman2])\n",
    "            sim_matrix[woman1_index][woman2_index] = input_token_dictionary[woman1].similarity(input_token_dictionary[woman2])\n",
    "    np.fill_diagonal(sim_matrix, 0)\n",
    "    \n",
    "    return sim_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "women2women_cosine_sim_matrix = build_sim_matrix(women_token_dictionary, len(women_names), women_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Code taken from A5\n",
    "def get_ranked_women(input_woman, input_sim_matrix, input_women_names) :\n",
    "#     print(input_sim_matrix)\n",
    "    '''Return sorted rankings (most to least similar) of women as \n",
    "        a list of two-element tuples, where the first element is the \n",
    "        woman's name and the second element is the similarity score\n",
    "    '''\n",
    "    \n",
    "    # Get index from woman's name\n",
    "    idx = input_women_names.index(input_woman)\n",
    "    \n",
    "    # Get list of similarity scores for woman\n",
    "    score_lst = input_sim_matrix[idx]\n",
    "    women_score_lst = [(input_women_names[index], score) for index, score in enumerate(score_lst)]\n",
    "    \n",
    "    # Do not account for woman herself in ranking\n",
    "    women_score_lst = women_score_lst[:idx] + women_score_lst[idx+1:]\n",
    "    \n",
    "    # Sort rankings by score (most similar to least similar)\n",
    "    women_score_lst = sorted(women_score_lst, key=lambda x: -x[1])\n",
    "    \n",
    "    # Only returning top 5!\n",
    "    return women_score_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a 'top 5 most similar' list of woman for every woman\n",
    "def create_top_5_dict_women(input_sim_mat, num_women, input_women_names):\n",
    "    women_top5 = {}\n",
    "    \n",
    "    # Loop through each woman\n",
    "    for woman in range(num_women):\n",
    "        similarity_list = get_ranked_women(input_women_names[woman], input_sim_mat, input_women_names)\n",
    "        similarity_list = [x[0] for x in similarity_list]\n",
    "        women_top5[input_women_names[woman]] = similarity_list\n",
    "        \n",
    "    return women_top5\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_dict_women2women = create_top_5_dict_women(women2women_cosine_sim_matrix, len(women_names), women_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Olabisi Ugbebor',\n",
       " u'Anne Greenbaum',\n",
       " u'H\\xe9l\\xe8ne Esnault',\n",
       " u'Anu\\u0161ka Ferligoj',\n",
       " u'Marina Ratner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_dict_women2women['Rachel Kuske']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(top5_dict_women2women, open( \"top5_dict_women2women.pickle\", \"wb\" ), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query output for Cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load from the pickle\n",
    "pkl_file = open('myvectorizer.pickle', 'rb')\n",
    "vectorizer = pickle.load(pkl_file)\n",
    "matx = pickle.load(pkl_file)\n",
    "\n",
    "def cossim_scores(vectorizer, matx, query, data_dict):\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    sim_doc_scores = cosine_similarity(q_vec, matx)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    This next block is commented out because we are changing this function to return cosine sim scores\n",
    "    '''\n",
    "#     sim_docs = np.argsort(sim_doc_scores.flatten())[::-1]\n",
    "#     return_docs = []\n",
    "#     for hit in sim_docs:\n",
    "#         if sim_doc_scores[0][hit] > 0:\n",
    "# #             print(sim_doc_scores[0][hit])\n",
    "#             return_docs.append(data_dict[hit])\n",
    "#     if len(return_docs)>30:\n",
    "#         return_docs = return_docs[:30]\n",
    "\n",
    "    '''\n",
    "    Alright! back to business\n",
    "    '''\n",
    "    return sim_doc_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cossim_scores(vectorizer, matx, u\"lalala\", deduped_women))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our Spacy Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Important!</span>\n",
    "The next block takes a while to run, but it's been pickled! Don't run the next 2 code blocks. Instead, just compile spacy_sim and run the block after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN!!\n",
    "spacy_array = np.zeros((len(deduped_women),384))\n",
    "for index, woman in enumerate(deduped_women):\n",
    "#     print(index)\n",
    "    wom_vec = nlp(woman['summary']).vector\n",
    "    if len(wom_vec) != 384:\n",
    "        wom_vec = np.zeros(384)\n",
    "    spacy_array[index] = wom_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle here\n",
    "# DON'T RUN THIS EITHER!\n",
    "with open(\"spacyarray.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(spacy_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacysim_scores(spacy_mat, query, data_dict):\n",
    "    '''\n",
    "    spacy_mat: numpy array of documents by spacy vectors\n",
    "    \n",
    "    query: query string\n",
    "    \n",
    "    RETURNS\n",
    "    \n",
    "    '''\n",
    "    q = nlp(query)\n",
    "    sim_scores = np.dot(spacy_mat, q.vector)/((np.linalg.norm(spacy_mat)*np.linalg.norm(q.vector))+1)\n",
    "    \n",
    "    '''\n",
    "    commenting out this next part because we are changing this to return sim scores\n",
    "    '''\n",
    "#     sim_docs = np.argsort(sim_scores)[::-1]\n",
    "#     return_docs = []\n",
    "#     for hit in sim_docs[0:30]:\n",
    "#         return_docs.append(data_dict[hit])\n",
    "\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00534153, 0.00482543, 0.0055273 , ..., 0.00613358, 0.00548279,\n",
       "       0.00497551])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file_spacy = open('spacyarray.pickle', 'rb')\n",
    "spacy_array = pickle.load(pkl_file_spacy)\n",
    "\n",
    "spacysim_scores(spacy_array, u\"lalala\", deduped_women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining our sim measures\n",
    "Spacy similarity scores + cosine similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_query(cossim_arr, spacysim_arr, data_dict):\n",
    "    '''\n",
    "    cossim_arr : 1D array of cosine similarity scores\n",
    "    spacysim_arr: 1D array of spacy similarity scores\n",
    "    data_dict: list of dicts (women)\n",
    "    \n",
    "    returns: TUPLE!\n",
    "    \n",
    "    return_docs : list of dict with top 30 women (keys are name, summary, views)\n",
    "    \n",
    "    '''\n",
    "    cosine_used = True\n",
    "    spacy_used = True\n",
    "    if max(cossim_arr) == 0:\n",
    "        cosine_used = False\n",
    "    if max(spacysim_arr) == 0:\n",
    "        spacy_used = False\n",
    "    weighted_scores = cossim_arr*0.7 + spacysim_arr*0.3\n",
    "    print(sum(weighted_scores))\n",
    "    sim_docs = np.argsort(weighted_scores)[::-1]\n",
    "    return_docs = []\n",
    "    for hit in sim_docs[0:30]:\n",
    "        print(hit)\n",
    "        print(data_dict[hit])\n",
    "        return_docs.append(data_dict[hit])\n",
    "    return return_docs, cosine_used, spacy_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.39401132525781\n",
      "5.78840740399188\n"
     ]
    }
   ],
   "source": [
    "query = u\"researched cancer\"\n",
    "s = spacysim_scores(spacy_array, query, deduped_women)\n",
    "print(sum(s))\n",
    "c = cossim_scores(vectorizer, matx, query, deduped_women)\n",
    "print(sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'name': u'Danielle Feinberg', u'summary': u'Danielle Feinberg is an American cinematographer and Director of Photography for Lighting at Pixar. She directed lighting for the Academy Award-winning films WALL-E and Brave and recently released Pixar film, Coco.'}\n",
      "{u'url': u'https://en.wikipedia.org/wiki/Mary_Anning', u'views': 794, u'name': u'Mary Anning', u'summary': u'Mary Anning (21 May 1799 \\u2013 9 March 1847) was an English fossil collector, dealer, and paleontologist who became known around the world for important finds she made in Jurassic marine fossil beds in the cliffs along the English Channel at Lyme Regis in the county of Dorset in Southwest England. Her findings contributed to important changes in scientific thinking about prehistoric life and the history of the Earth.\\nAnning searched for fossils in the area\\'s Blue Lias cliffs, particularly during the winter months when landslides exposed new fossils that had to be collected quickly before they were lost to the sea. She nearly died in 1833 during a landslide that killed her dog, Tray. Her discoveries included the first ichthyosaur skeleton correctly identified; the first two more complete plesiosaur skeletons found; the first pterosaur skeleton located outside Germany; and important fish fossils. Her observations played a key role in the discovery that coprolites, known as bezoar stones at the time, were fossilised faeces. She also discovered that belemnite fossils contained fossilised ink sacs like those of modern cephalopods. When geologist Henry De la Beche painted Duria Antiquior, the first widely circulated pictorial representation of a scene from prehistoric life derived from fossil reconstructions, he based it largely on fossils Anning had found, and sold prints of it for her benefit.\\nAnning did not fully participate in the scientific community of 19th-century Britain, who were mostly Anglican gentlemen. She struggled financially for much of her life. Her family was poor, and her father, a cabinetmaker, died when she was eleven.\\nShe became well known in geological circles in Britain, Europe, and America, and was consulted on issues of anatomy as well as about collecting fossils. Nonetheless, as a woman, she was not eligible to join the Geological Society of London and she did not always receive full credit for her scientific contributions. Indeed, she wrote in a letter: \"The world has used me so unkindly, I fear it has made me suspicious of everyone.\" The only scientific writing of hers published in her lifetime appeared in the Magazine of Natural History in 1839, an extract from a letter that Anning had written to the magazine\\'s editor questioning one of its claims.\\nAfter her death in 1847, her unusual life story attracted increasing interest. An uncredited author in All the Year Round, edited by Charles Dickens, wrote of her in 1865 that \"[t]he carpenter\\'s daughter has won a name for herself, and has deserved to win it.\" It has often been claimed that her story was the inspiration for the 1908 tongue-twister \"She sells seashells on the seallshore\" by Terry Sullivan. In 2010, one hundred and sixty-three years after her death, the Royal Society included Anning in a list of the ten British women who have most influenced the history of science.'}\n"
     ]
    }
   ],
   "source": [
    "# return_query(c, s, deduped_women_wack)\n",
    "print(deduped_women_wack[0])\n",
    "print(deduped_women[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
