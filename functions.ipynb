{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: msgpack-python in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.1 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python2.7/site-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Pegah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# the imports used in A2\n",
    "import re\n",
    "# import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "# import bs4\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# Imports that might help with various functionality\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "# Additional imports from A3\n",
    "from __future__ import print_function\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# import Levenshtein  # package python-Levenshtein\n",
    "\n",
    "# Additional imports from A5\n",
    "import nltk\n",
    "# nltk.download()\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not run these 2 blocks -- Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY package\n",
    "import spacy\n",
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"nlp.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(nlp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'nlp.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e47a303c2746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpkl_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'nlp.pickle'"
     ]
    }
   ],
   "source": [
    "pkl_nlp = open('nlp.pickle', 'rb')\n",
    "nlp = pickle.load(pkl_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each woman's entry is a dictionary with the following keys...\n",
      "[u'url', u'views', u'name', u'summary']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    women_summaries = json.load(f)\n",
    "\n",
    "print(\"Each woman's entry is a dictionary with the following keys...\")\n",
    "print(women_summaries[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVED FROM SPACY VERSION\n",
    "# creates a list of women to keep as a global variable\n",
    "women_names = list()\n",
    "for i in range(len(women_summaries)):\n",
    "    name = women_summaries[i]['name']\n",
    "    end_index = name.find('(')\n",
    "    if end_index != -1 and name[: (end_index-1)] not in women_names :\n",
    "        women_names.append(name[: (end_index-1)])\n",
    "    elif end_index == -1 and name not in women_names :\n",
    "        women_names.append(name)\n",
    "        \n",
    "# print(women_names)\n",
    "# women_names is a list of the names of women in order of the JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_names = []\n",
    "deduped_women = list(women_summaries)\n",
    "for woman in deduped_women:\n",
    "    if woman['name'] not in check_names:\n",
    "        check_names.append(woman['name'])\n",
    "    else:\n",
    "        deduped_women.remove(woman)\n",
    "import pickle\n",
    "pickle.dump(deduped_women, open( \"deduped_women.pickle\", \"wb\" ), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Dictionary\n",
    "For building token dictionary for W2W matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary that maps woman to the list of tokens associated with her\n",
    "# {'woman1' : ['token1', 'token2', ... 'tokenn']}\n",
    "def build_spacy_token_dictionary(input_summaries, input_women_names):\n",
    "    # Builds a dictionary that maps each woman to their tokenized words (yes we've already done this but we want the spacy version)\n",
    "    women_token_dictionary = dict()\n",
    "    \n",
    "    for i in range(len(input_women_names)):\n",
    "        name = input_women_names[i]\n",
    "        summary = input_summaries[i]['summary']\n",
    "    \n",
    "        index_was = summary.find('was ')\n",
    "        index_is = summary.find('is ')\n",
    "        index_currently = summary.find('currently ')\n",
    "        if (index_was == -1 and index_is == -1):\n",
    "            summary2 = summary[index_currently:]\n",
    "        elif (index_was == -1 and index_currently == -1):\n",
    "            summary2 = summary[index_is:]\n",
    "        elif (index_is == -1 and index_currently == -1):\n",
    "            summary2 = summary[index_was:]\n",
    "        else :\n",
    "            index_min = min(index_is, index_was)\n",
    "            summary2 = summary[index_min:]\n",
    "            \n",
    "        # lowercase long string of summary\n",
    "        summary_lower = summary2.lower()\n",
    "        # doc object type\n",
    "        summary_nlp = nlp(summary_lower)\n",
    "        # type list\n",
    "        summary_list = [token.text for token in summary_nlp]\n",
    "        # type list, but without stop words\n",
    "        summary_token_filtered = [w for w in summary_list if w not in stopwords.words('english')]\n",
    "        \n",
    "        # convert to a string separated by spaces\n",
    "        summary_str = \" \".join(summary_token_filtered) \n",
    "        summary_nlp = nlp(summary_str)\n",
    "        women_token_dictionary[name] = summary_nlp\n",
    "    \n",
    "#     print(women_token_dictionary)\n",
    "    return women_token_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT BECAUSE TOKENIZER DOES THIS\n",
    "women_token_dictionary = build_spacy_token_dictionary(women_summaries, women_names)\n",
    "# # print(women_token_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer\n",
    "#### Includes pickling the vectorizer and the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df = 0.95)\n",
    "matx = vectorizer.fit_transform(map(lambda x: x[\"summary\"], deduped_women))\n",
    "\n",
    "import pickle\n",
    "with open(\"myvectorizer.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    pickle.dump(matx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sim_matrix(input_token_dictionary, input_len_names, input_women_names):\n",
    "    # Builds matrix using spacy that stores each woman's cosine similarity to each other\n",
    "    \n",
    "    sim_matrix = np.zeros(shape = (input_len_names, input_len_names))\n",
    "    \n",
    "    for woman1 in input_token_dictionary:\n",
    "        for woman2 in input_token_dictionary:\n",
    "            woman1_index = input_women_names.index(woman1)\n",
    "            woman2_index = input_women_names.index(woman2)\n",
    "#             print(input_token_dictionary[woman1])\n",
    "#             print(input_token_dictionary[woman2])\n",
    "            sim_matrix[woman1_index][woman2_index] = input_token_dictionary[woman1].similarity(input_token_dictionary[woman2])\n",
    "    np.fill_diagonal(sim_matrix, 0)\n",
    "    \n",
    "    return sim_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "women2women_cosine_sim_matrix = build_sim_matrix(women_token_dictionary, len(women_names), women_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Code taken from A5\n",
    "def get_ranked_women(input_woman, input_sim_matrix, input_women_names) :\n",
    "#     print(input_sim_matrix)\n",
    "    '''Return sorted rankings (most to least similar) of women as \n",
    "        a list of two-element tuples, where the first element is the \n",
    "        woman's name and the second element is the similarity score\n",
    "    '''\n",
    "    \n",
    "    # Get index from woman's name\n",
    "    idx = input_women_names.index(input_woman)\n",
    "    \n",
    "    # Get list of similarity scores for woman\n",
    "    score_lst = input_sim_matrix[idx]\n",
    "    women_score_lst = [(input_women_names[index], score) for index, score in enumerate(score_lst)]\n",
    "    \n",
    "    # Do not account for woman herself in ranking\n",
    "    women_score_lst = women_score_lst[:idx] + women_score_lst[idx+1:]\n",
    "    \n",
    "    # Sort rankings by score (most similar to least similar)\n",
    "    women_score_lst = sorted(women_score_lst, key=lambda x: -x[1])\n",
    "    \n",
    "    # Only returning top 5!\n",
    "    return women_score_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a 'top 5 most similar' list of woman for every woman\n",
    "def create_top_5_dict_women(input_sim_mat, num_women, input_women_names):\n",
    "    women_top5 = {}\n",
    "    \n",
    "    # Loop through each woman\n",
    "    for woman in range(num_women):\n",
    "        similarity_list = get_ranked_women(input_women_names[woman], input_sim_mat, input_women_names)\n",
    "        similarity_list = [x[0] for x in similarity_list]\n",
    "        women_top5[input_women_names[woman]] = similarity_list\n",
    "        \n",
    "    return women_top5\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_dict_women2women = create_top_5_dict_women(women2women_cosine_sim_matrix, len(women_names), women_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Olabisi Ugbebor',\n",
       " u'Anne Greenbaum',\n",
       " u'H\\xe9l\\xe8ne Esnault',\n",
       " u'Anu\\u0161ka Ferligoj',\n",
       " u'Marina Ratner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_dict_women2women['Rachel Kuske']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(top5_dict_women2women, open( \"top5_dict_women2women.pickle\", \"wb\" ), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query output for Cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load from the pickle\n",
    "pkl_file = open('myvectorizer.pickle', 'rb')\n",
    "vectorizer = pickle.load(pkl_file)\n",
    "matx = pickle.load(pkl_file)\n",
    "\n",
    "def cossim_scores(vectorizer, matx, query, data_dict):\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    sim_doc_scores = cosine_similarity(q_vec, matx)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    This next block is commented out because we are changing this function to return cosine sim scores\n",
    "    '''\n",
    "#     sim_docs = np.argsort(sim_doc_scores.flatten())[::-1]\n",
    "#     return_docs = []\n",
    "#     for hit in sim_docs:\n",
    "#         if sim_doc_scores[0][hit] > 0:\n",
    "# #             print(sim_doc_scores[0][hit])\n",
    "#             return_docs.append(data_dict[hit])\n",
    "#     if len(return_docs)>30:\n",
    "#         return_docs = return_docs[:30]\n",
    "\n",
    "    '''\n",
    "    Alright! back to business\n",
    "    '''\n",
    "    return sim_doc_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cossim_scores(vectorizer, matx, u\"lalala\", deduped_women))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our Spacy Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Important!</span>\n",
    "The next block takes a while to run, but it's been pickled! Don't run the next 2 code blocks. Instead, just compile spacy_sim and run the block after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN!!\n",
    "spacy_array = np.zeros((len(deduped_women),384))\n",
    "for index, woman in enumerate(deduped_women):\n",
    "#     print(index)\n",
    "    wom_vec = nlp(woman['summary']).vector\n",
    "    if len(wom_vec) != 384:\n",
    "        wom_vec = np.zeros(384)\n",
    "    spacy_array[index] = wom_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle here\n",
    "# DON'T RUN THIS EITHER!\n",
    "with open(\"spacyarray.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(spacy_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacysim_scores(spacy_mat, query, data_dict):\n",
    "    '''\n",
    "    spacy_mat: numpy array of documents by spacy vectors\n",
    "    \n",
    "    query: query string\n",
    "    \n",
    "    RETURNS\n",
    "    \n",
    "    '''\n",
    "    q = nlp(query)\n",
    "    sim_scores = np.dot(spacy_mat, q.vector)/((np.linalg.norm(spacy_mat)*np.linalg.norm(q.vector))+1)\n",
    "    \n",
    "    '''\n",
    "    commenting out this next part because we are changing this to return sim scores\n",
    "    '''\n",
    "#     sim_docs = np.argsort(sim_scores)[::-1]\n",
    "#     return_docs = []\n",
    "#     for hit in sim_docs[0:30]:\n",
    "#         return_docs.append(data_dict[hit])\n",
    "\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00534153, 0.00482543, 0.0055273 , ..., 0.00613358, 0.00548279,\n",
       "       0.00497551])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file_spacy = open('spacyarray.pickle', 'rb')\n",
    "spacy_array = pickle.load(pkl_file_spacy)\n",
    "\n",
    "spacysim_scores(spacy_array, u\"lalala\", deduped_women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining our sim measures\n",
    "Spacy similarity scores + cosine similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_query(cossim_arr, spacysim_arr, data_dict):\n",
    "    '''\n",
    "    cossim_arr : 1D array of cosine similarity scores\n",
    "    spacysim_arr: 1D array of spacy similarity scores\n",
    "    data_dict: list of dicts (women)\n",
    "    \n",
    "    returns: TUPLE!\n",
    "    \n",
    "    return_docs : list of dict with top 30 women (keys are name, summary, views)\n",
    "    \n",
    "    '''\n",
    "    cosine_used = True\n",
    "    spacy_used = True\n",
    "    if max(cossim_arr) == 0:\n",
    "        cosine_used = False\n",
    "    if max(spacysim_arr) == 0:\n",
    "        spacy_used = False\n",
    "    weighted_scores = cossim_arr*0.7 + spacysim_arr*0.3\n",
    "    sim_docs = np.argsort(weighted_scores)[::-1]\n",
    "    return_docs = []\n",
    "    for hit in sim_docs[0:30]:\n",
    "        return_docs.append(data_dict[hit])\n",
    "    return return_docs, cosine_used, spacy_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{u'name': u'Toniann Pitassi',\n",
       "   u'summary': u'Toniann Pitassi is a Canadian and American mathematician and computer scientist specializing in computational complexity theory.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Toniann_Pitassi',\n",
       "   u'views': 6},\n",
       "  {u'name': u'Line Rochefort',\n",
       "   u'summary': u'Line Rochefort is a Canadian scientist specializing in peatland ecology.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Line_Rochefort',\n",
       "   u'views': 1},\n",
       "  {u'name': u'Regula Tschumi',\n",
       "   u'summary': u'Regula Tschumi is a Swiss social anthropologist and art historian.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Regula_Tschumi',\n",
       "   u'views': 1},\n",
       "  {u'name': u'Seema Nanda',\n",
       "   u'summary': u'Seema Nanda is an Indian mathematician. In her research she applies mathematics to study problems in biology, engineering and finance. Her research interests are primarily in solving real world problems using mathematics and computations.\\n\\n',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Seema_Nanda',\n",
       "   u'views': 3},\n",
       "  {u'name': u'Lori A. Clarke',\n",
       "   u'summary': u'Lori A. Clarke is an American computer scientist noted for her research on software engineering.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Lori_A._Clarke',\n",
       "   u'views': 2},\n",
       "  {u'name': u'Chiara Nappi',\n",
       "   u'summary': u'Chiara R. Nappi is an Italian physicist. Her research areas have included mathematical physics, particle physics, and string theory.\\n\\n',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Chiara_Nappi',\n",
       "   u'views': 80},\n",
       "  {u'name': u'Coralie Colmez',\n",
       "   u'summary': u'Coralie Colmez is a French mathematician, tutor and author.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Coralie_Colmez',\n",
       "   u'views': 6},\n",
       "  {u'name': u'Valeria de Paiva',\n",
       "   u'summary': u'Valeria Correa Vaz de Paiva is a Brazilian mathematician, logician, and computer scientist associated with Nuance Communications. Her work includes research on logical approaches to computation, especially using category theory, knowledge representation and natural language semantics, and functional programming with a focus on foundations and type theories.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Valeria_de_Paiva',\n",
       "   u'views': 6},\n",
       "  {u'name': u'Manju Ray',\n",
       "   u'summary': u'Manju Ray is an Indian scientist in Molecular Enzymology and Cancer Biochemistry. She has done notable work in the development of anticancer drug and understanding of differentiation process of cells. Her interests cover tumor biochemistry and molecular enzymology.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Manju_Ray',\n",
       "   u'views': 4},\n",
       "  {u'name': u'Matilde Marcolli',\n",
       "   u'summary': u'Matilde Marcolli is an Italian mathematical physicist.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Matilde_Marcolli',\n",
       "   u'views': 15},\n",
       "  {u'name': u'Lucia Caporaso',\n",
       "   u'summary': u'Lucia Caporaso is an Italian mathematician, holding a professorship in mathematics at Roma Tre University. Her research includes work in algebraic geometry, arithmetic geometry, tropical geometry and enumerative geometry.\\n\\n',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Lucia_Caporaso',\n",
       "   u'views': 4},\n",
       "  {u'name': u'Barbara L. Osofsky',\n",
       "   u'summary': u\"Barbara L. Osofsky is a retired professor of mathematics at Rutgers University. Her research concerns abstract algebra. Osofsky's contributions to mathematics include her characterization of semisimple rings in terms of properties of cyclic modules. Osofsky also established a logical equivalence between the continuum hypothesis and statements about the global dimension of associative rings.\",\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Barbara_L._Osofsky',\n",
       "   u'views': 2},\n",
       "  {u'name': u'Louise Leakey',\n",
       "   u'summary': u'Princess Louise de Merode (n\\xe9e: Leakey, born 21 March 1972) is a Kenyan paleontologist and anthropologist. She conducts research and field work on human fossils in Eastern Africa.\\n\\n',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Louise_Leakey',\n",
       "   u'views': 59},\n",
       "  {u'name': u'Marika Moisseeff',\n",
       "   u'summary': u'Marika Moisseeff is a French social anthropologist and psychiatrist.\\nMoisseeff has done extensive research on sexes and procreation, initiation rites, youth, parent-child relations, and aboriginal society and culture in general. She is currently head of research at CNRS.\\n\\n',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Marika_Moisseeff',\n",
       "   u'views': 1},\n",
       "  {u'name': u'Solveig Nordstr\\xf6m',\n",
       "   u'summary': u'Solveig Nordstr\\xf6m is a Swedish archeologist.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Solveig_Nordstr%C3%B6m',\n",
       "   u'views': 1},\n",
       "  {u'name': u'Graciela Chichilnisky',\n",
       "   u'summary': u'Graciela Chichilnisky Is an Argentine American mathematical economist and an authority on climate change. She is a professor of economics at Columbia University.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Graciela_Chichilnisky',\n",
       "   u'views': 25},\n",
       "  {u'name': u'Tara S. Holm',\n",
       "   u'summary': u'Tara Suzanne Holm is a mathematician at Cornell University specializing in algebraic geometry and symplectic geometry.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Tara_S._Holm',\n",
       "   u'views': 2},\n",
       "  {u'name': u'Brunilde Sismondo Ridgway',\n",
       "   u'summary': u'Brunilde Sismondo Ridgway (born 1929 in Chieti) is an Italian archaeologist and specialist in ancient Greek sculpture.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Brunilde_Sismondo_Ridgway',\n",
       "   u'views': 3},\n",
       "  {u'name': u'Aparna Higgins',\n",
       "   u'summary': u'Aparna W. Higgins is a mathematician known for her encouragement of undergraduate mathematicians to participate in mathematical research. Higgins originally specialized in universal algebra, but her more recent research concerns graph theory, including graph pebbling and line graphs. She is a professor of mathematics at the University of Dayton.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Aparna_Higgins',\n",
       "   u'views': 3},\n",
       "  {u'name': u'Samar Minallah',\n",
       "   u'summary': u'Samar Minallah (Urdu: \\u062b\\u0645\\u0631 \\u0645\\u0646 \\u0627\\u0644\\u0644\\u06c1\\u202c\\u200e ALA-LC: S\\u0331amar min All\\u0101h IPA: [\\u02c8s\\u0259m\\u0259r m\\u026an \\u0259l\\u02c8l\\u0251\\u02d0h]) is a documentary filmmaker, and human rights activist from Pakistan.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Samar_Minallah',\n",
       "   u'views': 6},\n",
       "  {u'name': u'Rachel Justine Pries',\n",
       "   u'summary': u'Rachel Justine Pries is an American mathematician whose current research interests concern arithmetic geometry and Galois theory. Her earlier work involved semi-stable reduction, deformation theory, and formal patching of curves. She is a Fellow of the American Mathematical Society.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Rachel_Justine_Pries',\n",
       "   u'views': 2},\n",
       "  {u'name': u'Eucharia Oluchi Nwaichi',\n",
       "   u'summary': u'Eucharia Oluchi Nwaichi is a Nigerian environmental biochemist, soil scientist and toxicologist.\\nHer research interest focus on Waste management, Pollution prevention and Phytoremediation, which involves the treatment of environmental problems (bioremediation) through the use of local plants that mitigate the environmental problem without the need to excavate the contaminant material and dispose of it elsewhere. She is an expert in elimination of Toxic heavy metal such as cadmium, copper, mercury, lead and arsenic from contaminated soil.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Eucharia_Oluchi_Nwaichi',\n",
       "   u'views': 2},\n",
       "  {u'name': u'Irit Dinur',\n",
       "   u'summary': u'Irit Dinur (Hebrew: \\u05d0\\u05d9\\u05e8\\u05d9\\u05ea \\u05d3\\u05d9\\u05e0\\u05d5\\u05e8) is an Israeli mathematician. She is professor of computer science at the Weizmann Institute of Science. Her research is in foundations of computer science and in combinatorics, and especially in probabilistically checkable proofs and hardness of approximation.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Irit_Dinur',\n",
       "   u'views': 8},\n",
       "  {u'name': u'Lihadh Al-Gazali',\n",
       "   u'summary': u'Professor Lihadh Al-Gazali MBChB MSc FRCP FRCPCH is a professor in clinical genetics and paediatrics. Her main area of interest is identifying new inherited disorders in Arab populations clinically and at the molecular level.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Lihadh_Al-Gazali',\n",
       "   u'views': 7},\n",
       "  {u'name': u'Karen Ho',\n",
       "   u'summary': u'Karen Ho is an American anthropologist. She contributed to anthropological research in Wall Street culture.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Karen_Ho',\n",
       "   u'views': 17},\n",
       "  {u'name': u'Colette Moeglin',\n",
       "   u'summary': u'Colette Moeglin is a French mathematician, working in the field of automorphic forms, a topic at the intersection of number theory and representation theory.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Colette_Moeglin',\n",
       "   u'views': 2},\n",
       "  {u'name': u'Sandra Saouaf',\n",
       "   u'summary': u'Sandra Saouaf is an American immunologist who researches autoimmune diseases, such as rheumatoid arthritis, type 1 diabetes, multiple sclerosis and others.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Sandra_Saouaf',\n",
       "   u'views': 1},\n",
       "  {u'name': u'Patricia Barchas',\n",
       "   u'summary': u'Patricia Barchas was a scholar committed to the study of the interaction of social behavior and physiological processes. She was a pioneer in the study of brain electrical activity and hormonal function in social processes, such as the development of status hierarchies in small social groups.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Patricia_Barchas',\n",
       "   u'views': 1},\n",
       "  {u'name': u'Anna Maria Bietti Sestieri',\n",
       "   u'summary': u'Anna Maria Bietti Sestieri is a contemporary Italian archaeologist based at the Universit\\xe0 del Salento whose research focuses on Italian prehistory.',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Anna_Maria_Bietti_Sestieri',\n",
       "   u'views': 2},\n",
       "  {u'name': u'Catherine Namono',\n",
       "   u'summary': u'Catherine Namono is an archaeologist that specializes in the study of Rock art in Uganda',\n",
       "   u'url': u'https://en.wikipedia.org/wiki/Catherine_Namono',\n",
       "   u'views': 1}],\n",
       " False,\n",
       " True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = u\"lalala\"\n",
    "s = spacysim_scores(spacy_array, query, deduped_women)\n",
    "c = cossim_scores(vectorizer, matx, query, deduped_women)\n",
    "return_query(c, s, deduped_women)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
