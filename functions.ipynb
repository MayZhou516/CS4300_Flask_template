{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python2.7/site-packages (from spacy)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: msgpack-python in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.1 in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python2.7/site-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/site-packages (from nltk)\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Pegah/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# the imports used in A2\n",
    "import re\n",
    "# import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "# import bs4\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# Imports that might help with various functionality\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "# Additional imports from A3\n",
    "from __future__ import print_function\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# import Levenshtein  # package python-Levenshtein\n",
    "\n",
    "# Additional imports from A5\n",
    "import nltk\n",
    "# nltk.download()\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not run these 2 blocks -- Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY package\n",
    "import spacy\n",
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"nlp.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(nlp, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'nlp.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e47a303c2746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpkl_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nlp.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpkl_nlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'nlp.pickle'"
     ]
    }
   ],
   "source": [
    "pkl_nlp = open('nlp.pickle', 'rb')\n",
    "nlp = pickle.load(pkl_nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each woman's entry is a dictionary with the following keys...\n",
      "[u'url', u'views', u'name', u'summary']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    women_summaries = json.load(f)\n",
    "\n",
    "print(\"Each woman's entry is a dictionary with the following keys...\")\n",
    "print(women_summaries[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVED FROM SPACY VERSION\n",
    "# creates a list of women to keep as a global variable\n",
    "women_names = list()\n",
    "for i in range(len(women_summaries)):\n",
    "    name = women_summaries[i]['name']\n",
    "    end_index = name.find('(')\n",
    "    if end_index != -1 and name[: (end_index-1)] not in women_names :\n",
    "        women_names.append(name[: (end_index-1)])\n",
    "    elif end_index == -1 and name not in women_names :\n",
    "        women_names.append(name)\n",
    "        \n",
    "# print(women_names)\n",
    "# women_names is a list of the names of women in order of the JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_names = []\n",
    "deduped_women = list(women_summaries)\n",
    "for woman in deduped_women:\n",
    "    if woman['name'] not in check_names:\n",
    "        check_names.append(woman['name'])\n",
    "    else:\n",
    "        deduped_women.remove(woman)\n",
    "import pickle\n",
    "pickle.dump(deduped_women, open( \"deduped_women.pickle\", \"wb\" ), protocol=2)\n",
    "pkl_file_wmn = open('deduped_women-search.pickle', 'rb')\n",
    "deduped_women = pickle.load(pkl_file_wmn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file_wmn = open('deduped_women-search.pickle', 'rb')\n",
    "deduped_women = pickle.load(pkl_file_wmn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Dictionary\n",
    "For building token dictionary for W2W matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary that maps woman to the list of tokens associated with her\n",
    "# {'woman1' : ['token1', 'token2', ... 'tokenn']}\n",
    "def build_spacy_token_dictionary(input_summaries, input_women_names):\n",
    "    # Builds a dictionary that maps each woman to their tokenized words (yes we've already done this but we want the spacy version)\n",
    "    women_token_dictionary = dict()\n",
    "    \n",
    "    for i in range(len(input_women_names)):\n",
    "        name = input_women_names[i]\n",
    "        summary = input_summaries[i]['summary']\n",
    "    \n",
    "        index_was = summary.find('was ')\n",
    "        index_is = summary.find('is ')\n",
    "        index_currently = summary.find('currently ')\n",
    "        if (index_was == -1 and index_is == -1):\n",
    "            summary2 = summary[index_currently:]\n",
    "        elif (index_was == -1 and index_currently == -1):\n",
    "            summary2 = summary[index_is:]\n",
    "        elif (index_is == -1 and index_currently == -1):\n",
    "            summary2 = summary[index_was:]\n",
    "        else :\n",
    "            index_min = min(index_is, index_was)\n",
    "            summary2 = summary[index_min:]\n",
    "            \n",
    "        # lowercase long string of summary\n",
    "        summary_lower = summary2.lower()\n",
    "        # doc object type\n",
    "        summary_nlp = nlp(summary_lower)\n",
    "        # type list\n",
    "        summary_list = [token.text for token in summary_nlp]\n",
    "        # type list, but without stop words\n",
    "        summary_token_filtered = [w for w in summary_list if w not in stopwords.words('english')]\n",
    "        \n",
    "        # convert to a string separated by spaces\n",
    "        summary_str = \" \".join(summary_token_filtered) \n",
    "        summary_nlp = nlp(summary_str)\n",
    "        women_token_dictionary[name] = summary_nlp\n",
    "    \n",
    "#     print(women_token_dictionary)\n",
    "    return women_token_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT BECAUSE TOKENIZER DOES THIS\n",
    "women_token_dictionary = build_spacy_token_dictionary(women_summaries, women_names)\n",
    "# # print(women_token_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer\n",
    "#### Includes pickling the vectorizer and the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df = 0.95)\n",
    "matx = vectorizer.fit_transform(map(lambda x: x[\"summary\"], deduped_women))\n",
    "\n",
    "import pickle\n",
    "with open(\"myvectorizer.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    pickle.dump(matx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sim_matrix(input_token_dictionary, input_len_names, input_women_names):\n",
    "    # Builds matrix using spacy that stores each woman's cosine similarity to each other\n",
    "    \n",
    "    sim_matrix = np.zeros(shape = (input_len_names, input_len_names))\n",
    "    \n",
    "    for woman1 in input_token_dictionary:\n",
    "        for woman2 in input_token_dictionary:\n",
    "            woman1_index = input_women_names.index(woman1)\n",
    "            woman2_index = input_women_names.index(woman2)\n",
    "#             print(input_token_dictionary[woman1])\n",
    "#             print(input_token_dictionary[woman2])\n",
    "            sim_matrix[woman1_index][woman2_index] = input_token_dictionary[woman1].similarity(input_token_dictionary[woman2])\n",
    "    np.fill_diagonal(sim_matrix, 0)\n",
    "    \n",
    "    return sim_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "women2women_cosine_sim_matrix = build_sim_matrix(women_token_dictionary, len(women_names), women_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Code taken from A5\n",
    "def get_ranked_women(input_woman, input_sim_matrix, input_women_names) :\n",
    "#     print(input_sim_matrix)\n",
    "    '''Return sorted rankings (most to least similar) of women as \n",
    "        a list of two-element tuples, where the first element is the \n",
    "        woman's name and the second element is the similarity score\n",
    "    '''\n",
    "    \n",
    "    # Get index from woman's name\n",
    "    idx = input_women_names.index(input_woman)\n",
    "    \n",
    "    # Get list of similarity scores for woman\n",
    "    score_lst = input_sim_matrix[idx]\n",
    "    women_score_lst = [(input_women_names[index], score) for index, score in enumerate(score_lst)]\n",
    "    \n",
    "    # Do not account for woman herself in ranking\n",
    "    women_score_lst = women_score_lst[:idx] + women_score_lst[idx+1:]\n",
    "    \n",
    "    # Sort rankings by score (most similar to least similar)\n",
    "    women_score_lst = sorted(women_score_lst, key=lambda x: -x[1])\n",
    "    \n",
    "    # Only returning top 5!\n",
    "    return women_score_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a 'top 5 most similar' list of woman for every woman\n",
    "def create_top_5_dict_women(input_sim_mat, num_women, input_women_names):\n",
    "    women_top5 = {}\n",
    "    \n",
    "    # Loop through each woman\n",
    "    for woman in range(num_women):\n",
    "        similarity_list = get_ranked_women(input_women_names[woman], input_sim_mat, input_women_names)\n",
    "        similarity_list = [x[0] for x in similarity_list]\n",
    "        women_top5[input_women_names[woman]] = similarity_list\n",
    "        \n",
    "    return women_top5\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_dict_women2women = create_top_5_dict_women(women2women_cosine_sim_matrix, len(women_names), women_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Olabisi Ugbebor',\n",
       " u'Anne Greenbaum',\n",
       " u'H\\xe9l\\xe8ne Esnault',\n",
       " u'Anu\\u0161ka Ferligoj',\n",
       " u'Marina Ratner']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_dict_women2women['Rachel Kuske']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(top5_dict_women2women, open( \"top5_dict_women2women.pickle\", \"wb\" ), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query output for Cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load from the pickle\n",
    "pkl_file = open('myvectorizer.pickle', 'rb')\n",
    "vectorizer = pickle.load(pkl_file)\n",
    "matx = pickle.load(pkl_file)\n",
    "\n",
    "def cossim_scores(vectorizer, matx, query, data_dict):\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    sim_doc_scores = cosine_similarity(q_vec, matx)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    This next block is commented out because we are changing this function to return cosine sim scores\n",
    "    '''\n",
    "#     sim_docs = np.argsort(sim_doc_scores.flatten())[::-1]\n",
    "#     return_docs = []\n",
    "#     for hit in sim_docs:\n",
    "#         if sim_doc_scores[0][hit] > 0:\n",
    "# #             print(sim_doc_scores[0][hit])\n",
    "#             return_docs.append(data_dict[hit])\n",
    "#     if len(return_docs)>30:\n",
    "#         return_docs = return_docs[:30]\n",
    "\n",
    "    '''\n",
    "    Alright! back to business\n",
    "    '''\n",
    "    return sim_doc_scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(cossim_scores(vectorizer, matx, u\"lalala\", deduped_women))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our Spacy Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Important!</span>\n",
    "The next block takes a while to run, but it's been pickled! Don't run the next 2 code blocks. Instead, just compile spacy_sim and run the block after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN!!\n",
    "spacy_array = np.zeros((len(deduped_women),384))\n",
    "for index, woman in enumerate(deduped_women):\n",
    "#     print(index)\n",
    "    wom_vec = nlp(woman['summary']).vector\n",
    "    if len(wom_vec) != 384:\n",
    "        wom_vec = np.zeros(384)\n",
    "    spacy_array[index] = wom_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle here\n",
    "# DON'T RUN THIS EITHER!\n",
    "with open(\"spacyarray.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(spacy_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacysim_scores(spacy_mat, query, data_dict):\n",
    "    '''\n",
    "    spacy_mat: numpy array of documents by spacy vectors\n",
    "    \n",
    "    query: query string\n",
    "    \n",
    "    RETURNS\n",
    "    \n",
    "    '''\n",
    "    q = nlp(query)\n",
    "    sim_scores = np.dot(spacy_mat, q.vector)/((np.linalg.norm(spacy_mat)*np.linalg.norm(q.vector))+1)\n",
    "    \n",
    "    '''\n",
    "    commenting out this next part because we are changing this to return sim scores\n",
    "    '''\n",
    "#     sim_docs = np.argsort(sim_scores)[::-1]\n",
    "#     return_docs = []\n",
    "#     for hit in sim_docs[0:30]:\n",
    "#         return_docs.append(data_dict[hit])\n",
    "\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00543635, 0.00578734, 0.00486329, ..., 0.00678422, 0.00482919,\n",
       "       0.00523706])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl_file_spacy = open('spacyarray.pickle', 'rb')\n",
    "spacy_array = pickle.load(pkl_file_spacy)\n",
    "\n",
    "spacysim_scores(spacy_array, u\"lalala\", deduped_women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining our sim measures\n",
    "Spacy similarity scores + cosine similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_query(cossim_arr, spacysim_arr, data_dict):\n",
    "    '''\n",
    "    cossim_arr : 1D array of cosine similarity scores\n",
    "    spacysim_arr: 1D array of spacy similarity scores\n",
    "    data_dict: list of dicts (women)\n",
    "    \n",
    "    returns: TUPLE!\n",
    "    \n",
    "    return_docs : list of dict with top 30 women (keys are name, summary, views)\n",
    "    \n",
    "    '''\n",
    "    cosine_used = True\n",
    "    spacy_used = True\n",
    "    if max(cossim_arr) == 0:\n",
    "        cosine_used = False\n",
    "    if max(spacysim_arr) == 0:\n",
    "        spacy_used = False\n",
    "    weighted_scores = cossim_arr*0.7 + spacysim_arr*0.3\n",
    "    print(sum(weighted_scores))\n",
    "    sim_docs = np.argsort(weighted_scores)[::-1]\n",
    "    return_docs = []\n",
    "    for hit in sim_docs[0:30]:\n",
    "        print(hit)\n",
    "        print(data_dict[hit])\n",
    "        return_docs.append(data_dict[hit])\n",
    "    return return_docs, cosine_used, spacy_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.392535167727555\n",
      "5.788407403991879\n"
     ]
    }
   ],
   "source": [
    "query = u\"researched cancer\"\n",
    "s = spacysim_scores(spacy_array, query, deduped_women)\n",
    "print(sum(s))\n",
    "c = cossim_scores(vectorizer, matx, query, deduped_women)\n",
    "print(sum(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'name': u'Lai-Sang Young', u'summary': u'Lai-Sang Lily Young (Chinese: \\u694a\\u9e97\\u7b19, born 1952) is a mathematician from Hong Kong, who holds the Henry & Lucy Moses Professorship of Science and is a professor of mathematics and neural science at the Courant Institute of Mathematical Sciences of New York University. Her research interests include dynamical systems, ergodic theory, chaos theory, probability theory, statistical mechanics, and neuroscience. She is particularly known for introducing the method of Markov returns in 1998, which she used to prove exponential correlation delay in Sinai billiards and other hyperbolic dynamical systems.'}\n",
      "{u'name': u'Laura Person', u'summary': u'Laura J. Person is an American mathematician specializing in low-dimensional topology. She is a Distinguished Teaching Professor of Mathematics at the State University of New York at Potsdam.'}\n"
     ]
    }
   ],
   "source": [
    "# return_query(c, s, deduped_women_wack)\n",
    "print(deduped_women_wack[100])\n",
    "print(deduped_women[44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
