{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jericahuang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# the imports used in A2\n",
    "import re\n",
    "# import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import bs4\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Imports that might help with various functionality\n",
    "import functools\n",
    "import operator\n",
    "import pickle\n",
    "\n",
    "# Additional imports from A3\n",
    "from __future__ import print_function\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import Levenshtein  # package python-Levenshtein\n",
    "\n",
    "# Additional imports from A5\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Ensure that your kernel is using Python3\n",
    "assert sys.version_info.major == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2477\n"
     ]
    }
   ],
   "source": [
    "# import the json\n",
    "import json\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    women_summaries = json.load(f)\n",
    "\n",
    "print(len(women_summaries))\n",
    "# print(women_summaries)\n",
    "# print(women_summaries[0]['name'])\n",
    "# print(women_summaries[0]['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/Margaret_Hamilton_(scientist)\n"
     ]
    }
   ],
   "source": [
    "# creates a list of women to keep as a global variable\n",
    "women_names = list()\n",
    "women_name_to_data = {}\n",
    "\n",
    "for i in range(len(women_summaries)):\n",
    "    name = women_summaries[i]['name']\n",
    "    end_index = name.find('(')\n",
    "    if end_index != -1 and name[: (end_index-1)] not in women_names :\n",
    "        women_names.append(name[: (end_index-1)])\n",
    "        women_name_to_data[name[: (end_index-1)]] = women_summaries[i]\n",
    "    elif end_index == -1 and name not in women_names :\n",
    "        women_names.append(name)\n",
    "        women_name_to_data[name] = women_summaries[i]\n",
    "        \n",
    "print(women_name_to_data[\"Margaret Hamilton\"][\"url\"])\n",
    "pickle.dump(women_name_to_data, open( \"women_name_to_data.pickle\", \"wb\" ), protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4382d6e1a01a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mfirst_sent_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sent_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mfiltered_first_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_sent_tok\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mregex_non_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\W]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-4382d6e1a01a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTreebankWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mfirst_sent_tok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_sent_lower\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mfiltered_first_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfirst_sent_tok\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mregex_non_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\W]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/corpus/reader/wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[1;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mline_tokenize\u001b[0;34m(text, blanklines)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblanklines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'discard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mLineTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblanklines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/nltk/tokenize/simple.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# If requested, strip off blank lines.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blanklines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'discard-eof'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# creates a dictionary {'name', [list of words in the summary]}\n",
    "women_dict_1sent = dict()\n",
    "# for i in range(len(women_summaries)):\n",
    "#     name = women_summaries[i]['name']\n",
    "for i in range(len(women_names)):\n",
    "    name = women_names[i]\n",
    "    summary = women_summaries[i]['summary']\n",
    "    # to do: \n",
    "    # hardcoded the words that we're looking for\n",
    "    # change to regex later time\n",
    "    index_was = summary.find('was ')\n",
    "    index_is = summary.find('is ')\n",
    "    index_currently = summary.find('currently ')\n",
    "    if (index_was == -1 and index_is == -1):\n",
    "        summary2 = summary[index_currently:]\n",
    "        # index_period = summary2.find('.')\n",
    "        # first_sent = summary2[:index_period]\n",
    "    elif (index_was == -1 and index_currently == -1):\n",
    "        summary2 = summary[index_is:]\n",
    "        # index_period = summary2.find('.')\n",
    "        # first_sent = summary2[:index_period]\n",
    "    elif (index_is == -1 and index_currently == -1):\n",
    "        summary2 = summary[index_was:]\n",
    "        # index_period = summary2.find('.')\n",
    "        # first_sent = summary2[:index_period]\n",
    "    else :\n",
    "        index_min = min(index_is, index_was)\n",
    "        summary2 = summary[index_min:]\n",
    "        # index_period = summary2.find('.')\n",
    "        # first_sent = summary2[:index_period]\n",
    "    # might want to change if we use ML that finds proper nouns\n",
    "    # implement a stemmer\n",
    "    first_sent_lower = summary2.lower()\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    first_sent_tok = tokenizer.tokenize(first_sent_lower)\n",
    "    filtered_first_sent = [w for w in first_sent_tok if w not in stopwords.words('english')]\n",
    "    \n",
    "    regex_non_word = re.compile(r'[^\\W]')\n",
    "    filtered_final = filter(regex_non_word.search, filtered_first_sent)\n",
    "\n",
    "    women_dict_1sent[name] = list(filtered_final)\n",
    "    \n",
    "print(women_dict_1sent)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# women_names_both = list()\n",
    "# counter = 0\n",
    "# for woman in women_names :\n",
    "#     if woman not in women_dict_1sent : \n",
    "#         print(\"JFDSLKJFLSDKJFLDSJFSD\")\n",
    "\n",
    "# print(len(set(women_names)))\n",
    "# print(len(women_dict_1sent))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumWords(text): \n",
    "    wordDict = dict()\n",
    "    for word in text :\n",
    "        if word in wordDict :\n",
    "            wordDict[word] = wordDict[word] + 1\n",
    "        else :\n",
    "            wordDict[word] = 1\n",
    "    return (wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the global variable of what is equivalent to good_types\n",
    "unique_word_lst = list()\n",
    "for woman in women_dict_1sent :\n",
    "    summary = women_dict_1sent[woman]\n",
    "    for word in summary :\n",
    "        if word not in unique_word_lst :\n",
    "            unique_word_lst.append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a np array where the rows are women according to the list of women names\n",
    "# columns are rows according to the list of unique words\n",
    "def create_word_freq_array(input_women_dict_1sent, input_women_names, input_num_women, input_unique_word_lst):\n",
    "    dict_freq = dict()\n",
    "\n",
    "    for woman in input_women_dict_1sent :\n",
    "        dict_freq[woman] = sumWords(input_women_dict_1sent[woman])\n",
    "    \n",
    "    np_with_freq = np.zeros(shape = (len(dict_freq), len(input_unique_word_lst)))\n",
    "    i = 0\n",
    "    for woman in input_women_names :\n",
    "#         print(woman)\n",
    "        if woman in dict_freq :\n",
    "            j = 0\n",
    "            for word in input_unique_word_lst :\n",
    "                if word in dict_freq[woman] :\n",
    "#                     print(j)\n",
    "                    np_with_freq[i][j] = dict_freq[woman][word]\n",
    "                j = j + 1\n",
    "            i = i + 1\n",
    "#     print(np_with_freq[0][0])\n",
    "    return np_with_freq\n",
    "#     print(input_unique_word_lst.index('chogha'))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_array = create_word_freq_array(women_dict_1sent, women_names, len(women_dict_1sent), unique_word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file is ordered from the homeworks that we've worked on\n",
    "# so that we can easily go back to the specific assignment if we need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Need to account for a woman's profile mentioning another woman's name\n",
    "\n",
    "def create_j_sim_mat_women(input_num_women, input_word_freq_array, input_unique_word_lst):\n",
    "    arr = np.zeros(shape = (input_num_women, input_num_women))\n",
    "    \n",
    "    for (i, woman1) in enumerate(input_word_freq_array) :\n",
    "        for (j, woman2) in enumerate(input_word_freq_array) :\n",
    "            s1 = np.nonzero(woman1)\n",
    "            s2 = np.nonzero(woman2)\n",
    "            intersect = np.intersect1d(s1, s2)\n",
    "            union = np.union1d(np.array(s1).flatten(), np.array(s2).flatten())\n",
    "            if len(union) > 0 :\n",
    "                arr[i][j] = len(intersect)/len(union)\n",
    "                \n",
    "    np.fill_diagonal(arr, 0)\n",
    "    \n",
    "#     print(np.sum(arr[100:]))\n",
    "    return arr\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_sim_mat_women = create_j_sim_mat_women(len(women_names), word_freq_array, unique_word_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# make dictionary of top 5 women for each woman\n",
    "def create_top_5_dict_women(j_sim_mat, num_women):\n",
    "    women_top5 = {}\n",
    "    # Loop through each woman\n",
    "    for woman in range(num_women):\n",
    "        top_5 = []      \n",
    "        min_similarity = (-1, -1)\n",
    "                \n",
    "        for i in range(num_women):\n",
    "            if (len(top_5) < 5): # Can just add bc we don't have top 5 yet\n",
    "                top_5.append((i, j_sim_mat[woman][i])) # Stores (index, similarity)\n",
    "                min_similarity = min(top_5, key = lambda t: t[1]) # Grabs tuple with minimum value\n",
    "            else: # Only add similarity if it is greater than the minimum similarity\n",
    "                if (j_sim_mat[woman][i] > min_similarity[1]):\n",
    "                    top_5.remove(min_similarity) # Remove minimum similarity tuple\n",
    "                    top_5.append((i, j_sim_mat[woman][i])) # Stores new (index, similarity)\n",
    "                    min_similarity = min(top_5, key = lambda t: t[1]) # Grabs tuple with minimum value\n",
    "                    \n",
    "        # Sort tuple list of top 5\n",
    "        top_5.sort(key = operator.itemgetter(1))\n",
    "        # Descending order\n",
    "        top_5.reverse()\n",
    "        \n",
    "        top_5_names = []\n",
    "        \n",
    "        for k, v in top_5:\n",
    "            top_5_names.append(women_names[k])\n",
    "            \n",
    "        # Store woman (key) and list of top 5 most similar (value)\n",
    "        women_top5[women_names[woman]] = top_5_names\n",
    "        \n",
    "    return(women_top5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dictionary of top 5 women for each word\n",
    "def create_top_5_dict_word(word_freq_array, num_words, num_women):\n",
    "    word_top5 = {}\n",
    "    # Loop through each word\n",
    "    for word in range(len(word_freq_array.T)): # Iterates through words first\n",
    "        top_5 = []      \n",
    "        min_similarity = (-1, 0)\n",
    "                \n",
    "        for i in range(num_women):\n",
    "            if (word_freq_array.T[word][i] > min_similarity[1]):\n",
    "                if (len(top_5) < 5):\n",
    "                    top_5.append((i, word_freq_array.T[word][i])) # Stores new (index, word count)\n",
    "                    min_similarity = min(top_5, key = lambda t: t[1]) # Grabs tuple with minimum value\n",
    "                else:\n",
    "                    top_5.remove(min_similarity) # Remove minimum similarity tuple\n",
    "                    top_5.append((i, word_freq_array.T[word][i])) # Stores new (index, word count)\n",
    "                    min_similarity = min(top_5, key = lambda t: t[1]) # Grabs tuple with minimum value\n",
    "                    \n",
    "        # Sort tuple list of top 5\n",
    "        top_5.sort(key = operator.itemgetter(1))\n",
    "        # Descending order\n",
    "        top_5.reverse()\n",
    "        \n",
    "        top_5_names = []\n",
    "        \n",
    "        for k, v in top_5:\n",
    "            top_5_names.append(women_names[k])\n",
    "            \n",
    "        # Store word (key) and list of top 5 most similar (value)\n",
    "        word_top5[unique_word_lst[word]] = top_5_names\n",
    "        \n",
    "    return(word_top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_top_5_dict_women' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-13516eb7c11f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_5_dict_women\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_top_5_dict_women\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_sim_mat_women\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwomen_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtop_5_dict_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_top_5_dict_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_freq_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_word_lst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwomen_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_5_dict_women\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"top_5_dict_women.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_5_dict_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"top_5_dict_words.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_5_dict_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_top_5_dict_women' is not defined"
     ]
    }
   ],
   "source": [
    "top_5_dict_women = create_top_5_dict_women(j_sim_mat_women, len(women_names))\n",
    "top_5_dict_words = create_top_5_dict_word(word_freq_array, len(unique_word_lst), len(women_names))\n",
    "pickle.dump(top_5_dict_women, open( \"top_5_dict_women.pickle\", \"wb\" ), protocol=2)\n",
    "pickle.dump(top_5_dict_words, open( \"top_5_dict_words.pickle\", \"wb\" ), protocol=2)\n",
    "print(top_5_dict_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Mamie Phipps Clark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-af1d9195d293>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtop_5_dict_women\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"top_5_dict_women.pickle\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtop_5_dict_women\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Mamie Phipps Clark'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Mamie Phipps Clark'"
     ]
    }
   ],
   "source": [
    "top_5_dict_women = pickle.load( open( \"top_5_dict_women.pickle\", \"rb\" ) )\n",
    "print (top_5_dict_women['Mamie Phipps Clark'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 3 looked at Levenshtein distance (probably not a good idea imo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_query(query, top_5_dict_words):\n",
    "    if query in women_names: # EX: 'is similar to Mary Jackson'\n",
    "        return(top_5_dict_women[query])\n",
    "    else: # EX: 'worked at NASA'\n",
    "        return(top_5_dict_words[query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 4 looked at cosine similarity, which we can consider on top of jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary Jackson', 'Claudia Alexander']\n"
     ]
    }
   ],
   "source": [
    "print(return_query('nasa', top_5_dict_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
