{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- nltk: stop words package that's already implemented\n",
    "- Built-in similarity function for spacy, needs to be called on an nlp-type version of a string object\n",
    "- Spacy type objects & dont want to tokenize things\n",
    "- texty (TF-IDF vectorizer and spacy had a baby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in ./venv/lib/python2.7/site-packages (2.0.11)\n",
      "Requirement already satisfied: ujson>=1.35 in ./venv/lib/python2.7/site-packages (from spacy) (1.35)\n",
      "Requirement already satisfied: murmurhash<0.29,>=0.28 in ./venv/lib/python2.7/site-packages (from spacy) (0.28.0)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in ./venv/lib/python2.7/site-packages (from spacy) (1.31.2)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in ./venv/lib/python2.7/site-packages (from spacy) (0.2.7.1)\n",
      "Requirement already satisfied: regex==2017.4.5 in ./venv/lib/python2.7/site-packages (from spacy) (2017.4.5)\n",
      "Requirement already satisfied: pathlib in ./venv/lib/python2.7/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in ./venv/lib/python2.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in ./venv/lib/python2.7/site-packages (from spacy) (0.9.6)\n",
      "Requirement already satisfied: numpy>=1.7 in ./venv/lib/python2.7/site-packages (from spacy) (1.14.2)\n",
      "Requirement already satisfied: thinc<6.11.0,>=6.10.1 in ./venv/lib/python2.7/site-packages (from spacy) (6.10.2)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in ./venv/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in ./venv/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (4.23.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in ./venv/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.11.0)\n",
      "Requirement already satisfied: msgpack-python in ./venv/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.5.6)\n",
      "Requirement already satisfied: wrapt in ./venv/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.10.11)\n",
      "Requirement already satisfied: msgpack-numpy==0.4.1 in ./venv/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (0.4.1)\n",
      "Requirement already satisfied: termcolor in ./venv/lib/python2.7/site-packages (from thinc<6.11.0,>=6.10.1->spacy) (1.1.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in ./venv/lib/python2.7/site-packages (from cytoolz<0.9,>=0.8->thinc<6.11.0,>=6.10.1->spacy) (0.9.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in ./venv/lib/python2.7/site-packages (3.2.5)\n",
      "Requirement already satisfied: six in ./venv/lib/python2.7/site-packages (from nltk) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.0, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amandachen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named sklearn.metrics.pairwise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ec6c3463ef49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sklearn.metrics.pairwise"
     ]
    }
   ],
   "source": [
    "# the imports used in A2\n",
    "import re\n",
    "# import json\n",
    "from glob import glob\n",
    "import os\n",
    "from io import StringIO\n",
    "from itertools import groupby\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "# import bs4\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# Imports that might help with various functionality\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "# Additional imports from A3\n",
    "from __future__ import print_function\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# import Levenshtein  # package python-Levenshtein\n",
    "\n",
    "# Additional imports from A5\n",
    "import nltk\n",
    "# nltk.download()\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY package\n",
    "import spacy\n",
    "# nlp = spacy.load('en')\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each woman's entry is a dictionary with the following keys...\n",
      "[u'views', u'name', u'summary']\n"
     ]
    }
   ],
   "source": [
    "# import the json\n",
    "import json\n",
    "with open(\"data.json\", \"r\") as f:\n",
    "    women_summaries = json.load(f)\n",
    "    \n",
    "# PEGAH (From A5)\n",
    "print(\"Each woman's entry is a dictionary with the following keys...\")\n",
    "print(women_summaries[0].keys())\n",
    "\n",
    "deduped_women_summaries = {}\n",
    "for woman in women_summaries:\n",
    "    if woman['name'] not in deduped_women_summaries:\n",
    "        deduped_women_summaries[woman['name']] = woman['summary']\n",
    "    else:\n",
    "        deduped_women_summaries[woman['name']] += \"\\n\" + woman['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVED FROM SPACY VERSION\n",
    "# creates a list of women to keep as a global variable\n",
    "women_names = list()\n",
    "for i in range(len(women_summaries)):\n",
    "    name = women_summaries[i]['name']\n",
    "    end_index = name.find('(')\n",
    "    if end_index != -1 and name[: (end_index-1)] not in women_names :\n",
    "        women_names.append(name[: (end_index-1)])\n",
    "    elif end_index == -1 and name not in women_names :\n",
    "        women_names.append(name)\n",
    "        \n",
    "# print(women_names)\n",
    "# women_names is a list of the names of women in order of the JSON file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "deduped_women = [{'summary': summary, 'name': name} for (name, summary) in list(deduped_women_summaries.items())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Dictionary (Commented out for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a dictionary that maps woman to the list of tokens associated with her\n",
    "# {'woman1' : ['token1', 'token2', ... 'tokenn']}\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "def build_spacy_token_dictionary(input_summaries, input_women_names):\n",
    "    # Builds a dictionary that maps each woman to their tokenized words (yes we've already done this but we want the spacy version)\n",
    "    women_token_dictionary = dict()\n",
    "    \n",
    "    for i in range(len(input_women_names)):\n",
    "        name = input_women_names[i]\n",
    "        summary = input_summaries[i]['summary']\n",
    "    \n",
    "        index_was = summary.find('was ')\n",
    "        index_is = summary.find('is ')\n",
    "        index_currently = summary.find('currently ')\n",
    "        if (index_was == -1 and index_is == -1):\n",
    "            summary2 = summary[index_currently:]\n",
    "        elif (index_was == -1 and index_currently == -1):\n",
    "            summary2 = summary[index_is:]\n",
    "        elif (index_is == -1 and index_currently == -1):\n",
    "            summary2 = summary[index_was:]\n",
    "        else :\n",
    "            index_min = min(index_is, index_was)\n",
    "            summary2 = summary[index_min:]\n",
    "            \n",
    "        # lowercase long string of summary\n",
    "        summary_lower = summary2.lower()\n",
    "        # doc object type\n",
    "        summary_nlp = nlp(summary_lower)\n",
    "        # type list\n",
    "        summary_list = [token.text for token in summary_nlp]\n",
    "        # type list, but without stop words\n",
    "        summary_token_filtered = [w for w in summary_list if w not in stopwords.words('english')]\n",
    "        \n",
    "        # convert to a string separated by spaces\n",
    "        summary_str = \" \".join(summary_token_filtered) \n",
    "        summary_nlp = nlp(summary_str)\n",
    "        women_token_dictionary[name] = summary_nlp\n",
    "    \n",
    "#     print(women_token_dictionary)\n",
    "    return women_token_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMMENTED OUT BECAUSE TOKENIZER DOES THIS\n",
    "women_token_dictionary = build_spacy_token_dictionary(women_summaries, women_names)\n",
    "# # print(women_token_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorizer\n",
    "#### Includes pickling the vectorizer and the TF-IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sklearn.feature_extraction.text",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-20b78423db28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmatx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeduped_women\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sklearn.feature_extraction.text"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_df = 0.95)\n",
    "matx = vectorizer.fit_transform(map(lambda x: x[\"summary\"], deduped_women))\n",
    "\n",
    "import pickle\n",
    "with open(\"myvectorizer.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    pickle.dump(matx, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"is a mother\"\n",
    "# q_vec = vectorizer.transform([query])\n",
    "# sim_doc_scores = cosine_similarity(q_vec, matx)\n",
    "# # apply weighting here\n",
    "# # print(sum(sim_doc_scores.flatten()))\n",
    "# sim_docs = np.argsort(sim_doc_scores.flatten())[::-1]\n",
    "# sim_docs\n",
    "\n",
    "# JUST KIDDING!! WE DO THIS IN THE FUNCTION BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sim_matrix(input_token_dictionary, input_len_names, input_women_names):\n",
    "    # Builds matrix using spacy that stores each woman's cosine similarity to each other\n",
    "    \n",
    "    sim_matrix = np.zeros(shape = (input_len_names, input_len_names))\n",
    "    \n",
    "    for woman1 in input_token_dictionary:\n",
    "        for woman2 in input_token_dictionary:\n",
    "            woman1_index = input_women_names.index(woman1)\n",
    "            woman2_index = input_women_names.index(woman2)\n",
    "#             print(input_token_dictionary[woman1])\n",
    "#             print(input_token_dictionary[woman2])\n",
    "            sim_matrix[woman1_index][woman2_index] = input_token_dictionary[woman1].similarity(input_token_dictionary[woman2])\n",
    "    np.fill_diagonal(sim_matrix, 0)\n",
    "    \n",
    "    return sim_matrix\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "women2women_cosine_sim_matrix = build_sim_matrix(women_token_dictionary, len(women_names), women_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8791370558469825\n"
     ]
    }
   ],
   "source": [
    "print(women2women_cosine_sim_matrix[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.87913706 0.88600144 ... 0.77867681 0.88487536 0.76331609]\n",
      " [0.87913706 0.         0.86443903 ... 0.87234958 0.8744772  0.7776874 ]\n",
      " [0.88600144 0.86443903 0.         ... 0.75078776 0.86300737 0.84362826]\n",
      " ...\n",
      " [0.77867681 0.87234958 0.75078776 ... 0.         0.80007459 0.66842487]\n",
      " [0.88487536 0.8744772  0.86300737 ... 0.80007459 0.         0.84861321]\n",
      " [0.76331609 0.7776874  0.84362826 ... 0.66842487 0.84861321 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(women2women_cosine_sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "# Code taken from A5\n",
    "def get_ranked_women(input_woman, input_sim_matrix, input_women_names) :\n",
    "#     print(input_sim_matrix)\n",
    "    '''Return sorted rankings (most to least similar) of women as \n",
    "        a list of two-element tuples, where the first element is the \n",
    "        woman's name and the second element is the similarity score\n",
    "    '''\n",
    "    \n",
    "    # Get index from woman's name\n",
    "    idx = input_women_names.index(input_woman)\n",
    "    \n",
    "    # Get list of similarity scores for woman\n",
    "    score_lst = input_sim_matrix[idx]\n",
    "    women_score_lst = [(input_women_names[index], score) for index, score in enumerate(score_lst)]\n",
    "    \n",
    "    # Do not account for woman herself in ranking\n",
    "    women_score_lst = women_score_lst[:idx] + women_score_lst[idx+1:]\n",
    "    \n",
    "    # Sort rankings by score (most similar to least similar)\n",
    "    women_score_lst = sorted(women_score_lst, key=lambda x: -x[1])\n",
    "    \n",
    "    # Only returning top 5!\n",
    "    return women_score_lst[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a 'top 5 most similar' list of woman for every woman\n",
    "def create_top_5_dict_women(input_sim_mat, num_women, input_women_names):\n",
    "    women_top5 = {}\n",
    "    \n",
    "    # Loop through each woman\n",
    "    for woman in range(num_women):\n",
    "        similarity_list = get_ranked_women(input_women_names[woman], input_sim_mat, input_women_names)\n",
    "        similarity_list = [x[0] for x in similarity_list]\n",
    "        women_top5[input_women_names[woman]] = similarity_list\n",
    "        \n",
    "    return women_top5\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_dict_women2women = create_top_5_dict_women(women2women_cosine_sim_matrix, len(women_names), women_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Olabisi Ugbebor',\n",
       " u'Anne Greenbaum',\n",
       " u'H\\xe9l\\xe8ne Esnault',\n",
       " u'Karen Vousden',\n",
       " u'Anu\\u0161ka Ferligoj']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_dict_women2women['Rachel Kuske']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(top5_dict_women2women, open( \"top5_dict_women2women.pickle\", \"wb\" ), protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query output for Spacy (Commented out for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY OUTPUT RETURN -- commented out for now\n",
    "# def return_query(input_query, input_women_token_dictionary):\n",
    "#     # Given a query string, this function will return a list of women that are most similar to the query\n",
    "    \n",
    "#     query = nlp(input_query)\n",
    "#     results = [] # List that will eventually store name, similarity, popularity, and ultimate RANKING\n",
    "    \n",
    "#     # Ranking will be defined by: (1/popularity) + (0.5*similarity) + entity weighting\n",
    "    \n",
    "#     for woman in input_women_token_dictionary:\n",
    "#         similarity = query.similarity(input_women_token_dictionary[woman])\n",
    "#         if similarity > 0.2: # Arbitrary number, can play around with with\n",
    "#             results.append((woman, similarity, similarity)) # Currently similarity will act as the ranking\n",
    "            \n",
    "#     results.sort(key=lambda x: x[2]) # Ascending order of rankings\n",
    "#     results.reverse() # Descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query output for Cossim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Load from the pickle\n",
    "pkl_file = open('myvectorizer.pickle', 'rb')\n",
    "vectorizer = pickle.load(pkl_file)\n",
    "matx = pickle.load(pkl_file)\n",
    "\n",
    "def return_query(vectorizer, matx, query, data_dict):\n",
    "    q_vec = vectorizer.transform([query])\n",
    "    sim_doc_scores = cosine_similarity(q_vec, matx)\n",
    "#     print(sim_doc_scores)\n",
    "    # Apply weighting here!\n",
    "    sim_docs = np.argsort(sim_doc_scores.flatten())[::-1]\n",
    "    return_docs = []\n",
    "\n",
    "    for hit in sim_docs:\n",
    "        if sim_doc_scores[0][hit] > 0:\n",
    "#             print(sim_doc_scores[0][hit])\n",
    "            return_docs.append(data_dict[hit])\n",
    "    if len(return_docs)>30:\n",
    "        return_docs = return_docs[:30]\n",
    "    return return_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': u'Germaine Tillion',\n",
       "  'summary': u'Germaine Tillion (30 May 1907 \\u2013 18 April 2008) was a French ethnologist, best known for her work in Algeria in the 1950s on behalf of the French government. A member of the French resistance, she spent time in the Ravensbr\\xfcck concentration camp.'},\n",
       " {'name': u'Valerie Mizrahi',\n",
       "  'summary': u'Valerie Mizrahi (born 1958) is a South African molecular biologist.'},\n",
       " {'name': u'Lucie Randoin',\n",
       "  'summary': u'Lucie Gabrielle Randoin (11 May 1888 \\u2013 13 September 1960) was a French biologist, nutritionist, and hygienist. She was made a commander of the Legion of Honour in 1958 and is known for her research on vitamins.'},\n",
       " {'name': u'Yvonne Choquet-Bruhat',\n",
       "  'summary': u'Yvonne Choquet-Bruhat (French: [b\\u0281y.a]; born 29 December 1923 in Lille) is a French mathematician and physicist. She was the first woman to be elected to the Acad\\xe9mie des Sciences Fran\\xe7aise (\"French Academy of Sciences\") and is a Grand Officier of the L\\xe9gion d\\'honneur.\\n\\n'},\n",
       " {'name': u'Diana Marcela Bola\\xf1os Rodriguez',\n",
       "  'summary': u\"Diana Marcela Bola\\xf1os Rodr\\xedguez (born September 24 1981) is a marine biologist from Colombia, who has studied and classified various types of platyhelminths. She was a recipient of the L'Or\\xe9al-UNESCO Fellowship for Women in Science in 2010, was selected as Colombian biologist of the year in 2012, and in 2013 was named by the BBC as one of the top ten women in science in Latin America.\\nDiana Marcela Bola\\xf1os Rodr\\xedguez (born September 24 1981) is a marine biologist from Colombia, who has studied and classified various types of platyhelminths. She was a recipient of the L'Or\\xe9al-UNESCO Fellowship for Women in Science in 2010, was selected as Colombian biologist of the year in 2012, and in 2013 was named by the BBC as one of the top ten women in science in Latin America.\"},\n",
       " {'name': u'Laurence Tubiana',\n",
       "  'summary': u'Laurence Tubiana (born 1951) is a French economist and diplomat. She was appointed French ambassador for international climate negotiations in connection with the 2015 COP21 Climate Change Conference in Paris. She founded and has headed the Paris-based Institute of Sustainable Development and International Relations (IDDRI), is a professor at the Paris high-level education institute, Sciences Po, and has previously served as senior adviser on the environment to the former French Prime Minister Lionel Jospin. She has been responsible for conducting international environmental negotiations for the French government and has also been a member of the Economic Analysis Council (Conseil d\\u2019analyse \\xe9conomique) attached to the French Prime Minister\\u2019s office.'},\n",
       " {'name': u'Nicole Berline',\n",
       "  'summary': u'Nicole Berline (born 1944) is a French mathematician.'},\n",
       " {'name': u'Guillemette Andreu',\n",
       "  'summary': u'Guillemette Andreu-Lano\\xeb (born August 3, 1948 in Paris), is a French Egyptologist and archaeologist. A former member of the French Institute of Oriental Archaeology of Cairo, she has been a curator and director of the Department of Egyptian Antiquities of the Louvre Museum since May 2007.'},\n",
       " {'name': u'Gertrude Van Wagenen',\n",
       "  'summary': u'Gertrude L. Van Wagenen (1893 \\u2013 February 8, 1978) was an American biologist. She was also a collector of anatomical illustrations and models.'},\n",
       " {'name': u'Margarete Zuelzer',\n",
       "  'summary': u'Margarete Hedwig Zuelzer (2 February 1877 \\u2013 29 August 1943) was a German biologist and zoologist specializing in the study of protozoa.'},\n",
       " {'name': u'Reidun Twarock',\n",
       "  'summary': u'Reidun Twarock is a German-born mathematical biologist at the University of York. She is known for developing mathematical models of viruses based on higher-dimensional lattices.'},\n",
       " {'name': u'Nathalie Luca',\n",
       "  'summary': u'Nathalie Luca (born 1966) is a French research director at the French National Centre for Scientific Research (CNRS), an anthropologist and a sociologist of religions. She is deputy director of the Center for Interdisciplinary Studies of Religious Facts at the \\xc9cole des hautes \\xe9tudes en sciences sociales and co-editor-in-chief of the French review Archives de sciences sociales des religions with Pierre Lassave.\\nShe was a member of the French government agency monitoring and combatting cultic deviances MIVILUDES from March 2003 to November 2005. She resigned on the ground that she refused to participate in a predictable hardening of policy of this organization.\\nShe wrote many books on groups she defined as \"cults\" and is regularly interviewed in the media, and by anti-cult organizations on this issue. She said she is not in favour of the establishment of a list of cults.'},\n",
       " {'name': u'Cl\\xe9mence Royer',\n",
       "  'summary': u\"Cl\\xe9mence Royer (21 April 1830 \\u2013 6 February 1902) was a self-taught French scholar who lectured and wrote on economics, philosophy, science and feminism. She is best known for her controversial 1862 French translation of Charles Darwin's On the Origin of Species.\"},\n",
       " {'name': u'Claire Voisin',\n",
       "  'summary': u'Claire Voisin (born 4 March 1962) is a French mathematician known for her work in algebraic geometry.'},\n",
       " {'name': u'Coralie Colmez',\n",
       "  'summary': u'Coralie Colmez is a French mathematician, tutor and author.'},\n",
       " {'name': u'C\\xe9cile DeWitt-Morette',\n",
       "  'summary': u'C\\xe9cile Andr\\xe9e Paule DeWitt-Morette (21 December 1922 \\u2013 8 May 2017) was a French mathematician and physicist. She founded a summer school at Les Houches in the French Alps. For this and her publications, she was awarded the American Society of the French Legion of Honour 2007 Medal for Distinguished Achievement. Attendees at the summer school included over twenty students who would go on to be Nobel Prize winners, including Pierre-Gilles de Gennes, Georges Charpak, and Claude Cohen-Tannoudji, who identify the school for assisting in their success.'},\n",
       " {'name': u'Kathrine S. French',\n",
       "  'summary': u'Kathrine Story (\"Kay\") French (June 5, 1922 \\u2013 June 14, 2006) was an American anthropologist born in Illinois. Educated in California, she studied ceremonialism and naming practices on the Warm Springs Indian Reservation in the state of Oregon. She was married to fellow anthropologist David H. French.'},\n",
       " {'name': u'Mary Buckland',\n",
       "  'summary': u'Mary Morland Buckland (20 November 1797 \\u2013 30 November 1857) was a British palaeontologist, marine biologist and scientific illustrator.'},\n",
       " {'name': u'L\\xfacia Mendon\\xe7a Previato',\n",
       "  'summary': u\"L\\xfacia Mendon\\xe7a Previato (born 1949) is a Brazilian biologist. She was awarded the L'Or\\xe9al-UNESCO Awards for Women in Science in 2004 for her research into preventing Chagas disease.\"},\n",
       " {'name': u'Mariel V\\xe1zquez',\n",
       "  'summary': u'Mariel V\\xe1zquez is a Mexican mathematical biologist who specializes in the topology of DNA. She is a professor at the University of California, Davis, jointly affiliated with the departments of mathematics and of microbiology and molecular genetics.'},\n",
       " {'name': u'Mina J. Bissell',\n",
       "  'summary': u\"Mina J. Bissell is an Iranian-American biologist known for her research on breast cancer. In particular, she has studied the effects of a cell's microenvironment, including its extracellular matrix, on tissue function.\"},\n",
       " {'name': u'Anny Cazenave',\n",
       "  'summary': u\"Anny Cazenave (French pronunciation: [ani kaznav] ( listen)) is a French space geodesist and one of the pioneers in satellite altimetry. She works for the French space agency CNES and has been deputy director of the Laboratoire d'Etudes en Geophysique et Oceanographie Spatiale (LEGOS) at Observatoire Midi-Pyr\\xe9n\\xe9es in Toulouse since 1996. Since 2013, she is director of Earth sciences at the International Space Sciences institute (ISSI), in Bern (Switzerland).\\nAs one of the leading scientists in the joint French/American satellite altimetry missions TOPEX/Poseidon, Jason-1, and the Ocean Surface Topography Mission, she has contributed to a greater understanding of sea level rise caused by global warming. Cazenave is a member of the Intergovernmental Panel on Climate Change and was the lead author of the sea level sections for their fourth and fifth Assessment Reports.\"},\n",
       " {'name': u'Danielle Stordeur',\n",
       "  'summary': u'Danielle Stordeur is a French Archaeologist and Directeur de Recherche at the CNRS. She is also Director of the French Ministry of Foreign Affairs permanent mission to El Kowm-Mureybet (Syria), replacing Jacques Cauvin in 1993 until 2010, when Fr\\xe9d\\xe9ric Abb\\xe8s is due to take over this position.'},\n",
       " {'name': u'Jill P. Mesirov',\n",
       "  'summary': u'Jill P. Mesirov is an American mathematician, computer scientist, and computational biologist who was the associate director and chief informatics officer at the Eli and Edythe L. Broad Institute of MIT and Harvard. She also holds an adjunct faculty position at Boston University.'},\n",
       " {'name': u'Huguette Delavault',\n",
       "  'summary': u'Huguette Delavault (15 January 1924 \\u2013 2 April 2003) was a French mathematician, specializing in mathematical physics.'},\n",
       " {'name': u'Christiane Desroches Noblecourt',\n",
       "  'summary': u'Christiane Desroches Noblecourt (French pronunciation: [k\\u0281istjan d\\u025bs\\u0281\\u0254\\u0283 n\\u0254bl\\u0259ku\\u0281] ( listen); 17 November 1913 \\u2013 23 June 2011) was a French Egyptologist. She was the author of many books on Egyptian art and history and was also known for her role in the preservation of the Nubian temples from flooding caused by the Aswan Dam.'},\n",
       " {'name': u'Anne Rudloe',\n",
       "  'summary': u'Anne Rudloe (n\\xe9e Eidemiller, December 24, 1947 \\u2013 April 27, 2012) was an American marine biologist. She was the co-founder of the Gulf Specimen Marine Laboratory in Panacea, Florida.'},\n",
       " {'name': u'Marie Crous',\n",
       "  'summary': u'Marie Crous was a French mathematician. She introduced the decimal system to France in the 17th century.'},\n",
       " {'name': u'Marie Farge',\n",
       "  'summary': u\"Marie Farge (born 1953) is a French mathematician and physicist who works as a director of research at CNRS, the French National Centre for Scientific Research. She is known for her research on wavelets and turbulence in fluid mechanics.\\nFarge earned a master's degree from Stanford University in 1977, and a third cycle doctorate in physics from Paris Diderot University in 1980. After postdoctoral studies on a Fulbright Fellowship at Harvard University, she continued her studies at Pierre and Marie Curie University, where she completed a state doctorate in 1987. She has been a researcher at CNRS since 1981. She has also held short-term positions at many other universities, including being Sofia Kovaleskaia Chair of Mathematics in 1994\\u201395 at Kaiserslautern University.\\nFarge was the 1993 winner of the Poncelet Prize of the French Academy of Sciences. She became a member of the Academia Europaea in 2005, and a fellow of the American Physical Society in 2011.\"},\n",
       " {'name': u'Marie Lebour',\n",
       "  'summary': u'Marie Victoire Lebour (August 20, 1876 \\u2013 October 2, 1971) was a British marine biologist known for her study of the life cycles of various marine animals. She published more than 175 works during her long career.'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_query(vectorizer, matx, \"is a french biologist\", deduped_women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc1 = nlp(deduped_women[0]['summary'])\n",
    "# # print(doc1)\n",
    "# doc2 = nlp(deduped_women[1]['summary'])\n",
    "# # print(doc2)\n",
    "\n",
    "# arr = np.column_stack((doc1.vector,doc2.vector))\n",
    "\n",
    "# doc3 = nlp(u'is a cinematographer')\n",
    "\n",
    "\n",
    "# # print(deduped_women[0]['summary'])\n",
    "# # print(doc.vector)\n",
    "# # assert doc.vector.dtype == 'float32'\n",
    "# # len(doc.vector)\n",
    "# sim1 = np.dot(doc1.vector, doc3.vector)/(np.linalg.norm(doc1.vector)*np.linalg.norm(doc3.vector)) # cinema to query\n",
    "# sim2 = np.dot(doc2.vector, doc3.vector)/(np.linalg.norm(doc2.vector)*np.linalg.norm(doc3.vector)) # physicist to query\n",
    "\n",
    "# # print(doc1.similarity(doc3))\n",
    "# # print(doc2.similarity(doc3))\n",
    "\n",
    "# print(doc1)\n",
    "# print(sim1)\n",
    "# print()\n",
    "# print(doc2)\n",
    "# print(sim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making our Spacy Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Important!</span>\n",
    "The next block takes a while to run, but it's been pickled! Don't run the next 2 code blocks. Instead, just compile spacy_sim and run the block after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN!!\n",
    "spacy_array = np.zeros((len(deduped_women),300))\n",
    "for index, woman in enumerate(deduped_women):\n",
    "#     print(index)\n",
    "    wom_vec = nlp(woman['summary']).vector\n",
    "    if len(wom_vec) != 300:\n",
    "        wom_vec = np.zeros(300)\n",
    "    spacy_array[index] = wom_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle here\n",
    "# DON'T RUN THIS EITHER!\n",
    "with open(\"spacyarray.pickle\", \"wb+\") as f:\n",
    "    pickle.dump(spacy_array, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_sim(spacy_mat, query, data_dict):\n",
    "    '''\n",
    "    spacy_mat: numpy array of documents by spacy vectors\n",
    "    \n",
    "    query: query string\n",
    "    \n",
    "    RETURNS\n",
    "    \n",
    "    '''\n",
    "    q = nlp(query)\n",
    "    print(q)\n",
    "    sim_scores = np.dot(spacy_mat, q.vector)/(np.linalg.norm(spacy_mat)*np.linalg.norm(q.vector))\n",
    "    sim_docs = np.argsort(sim_scores)[::-1]\n",
    "    \n",
    "    return_docs = []\n",
    "    for hit in sim_docs[0:30]:\n",
    "        return_docs.append(data_dict[hit])\n",
    "    return return_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file_spacy = open('spacyarray.pickle', 'rb')\n",
    "spacy_array = pickle.load(pkl_file_spacy)\n",
    "\n",
    "# spacy_sim(spacy_array, \"won the nobel prize\", deduped_women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining our sim measures\n",
    "Spacy similarity scores + cosine similarity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next changes\n",
    "* Have our similarity function return the sim_scores numpy array, and then have another function that uses our weighting algorithm to return the top 30\n",
    "* Decide on TWE we want to use views in weighting if we're going to do a sort-by feature"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
